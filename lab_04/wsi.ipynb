{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole-slide image processing\n",
    "이번 실습에서는 Histopathology 이미지가 저장된 형태인 Whole-slide image (WSI) 분석을 수행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import openslide\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data_root_dir = \"/datasets/CAMELYON16\"\n",
    "slide_name = \"test_001\"\n",
    "slide_filepath = Path(data_root_dir) / f\"testing/images/{slide_name}.tif\"\n",
    "annotation_filepath = Path(data_root_dir) / f\"testing/lesion_annotations/{slide_name}.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSlide\n",
    "OpenSlide는 Whole-slide image 처리를 도와주는 라이브러리이다. 이를 이용하여 Whole slide 이미지를 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Opening Slide {slide_filepath}\")\n",
    "slide = openslide.open_slide(slide_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_slide_info(slide, slide_filepath):\n",
    "    print(\"Level count: %d\" % slide.level_count)\n",
    "    print(\"Level dimensions: \" + str(slide.level_dimensions))\n",
    "    print(\"Level downsamples: \" + str(slide.level_downsamples))\n",
    "    print(\"Slide dimensions (width, height): \" + str(slide.dimensions))\n",
    "    print(\"Format: \" + str(slide.detect_format(slide_filepath)))\n",
    "    print(\"Properties:\")\n",
    "    for prop_key in slide.properties.keys():\n",
    "        print(\"  Property: \" + str(prop_key) + \", value: \" + str(slide.properties.get(prop_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print_slide_info(slide, slide_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole slide image 파일은 아래와 같이 다양한 해상도의 이미지를 하나의 파일에 모두 포함하고 있습니다..\n",
    "\n",
    "<img src=\"resources/svs.jpg\" width=\"400\" align=\"middle\"/>\n",
    "\n",
    "이 이미지에는 총 9의 `level` 이미지가 존재합니다\n",
    "\n",
    "- `Level[0]`: 원본 이미지(Whole slide)를 그대로 담고 있으며, `downsample = 1`이다\n",
    "- `Level[1]`: 원본 이미지를 `downsample = 2`로 다운 샘플링을 한 이미지가 보관되어 있다.\n",
    "- `Level[2]`: 원본 이미지를 `downsample = 4`로 다운 샘플링을 한 이미지가 보관되어 있다.\n",
    "- ...\n",
    "- `Level[8]`: 가장 낮은 해상도의 이미지, `downsample = 256`, 이미지 크기 `(336, 350)`\n",
    "\n",
    "\n",
    "\n",
    "`slide.read_region((x, y), level, (width, height))`함수를 이용하여 원하는 영역의 이미지를 읽어올 수 있습니다\n",
    "- location (tuple) – `level = 0`에서의 left-top 픽셀 위치를 지정하는 튜플 `(x, y)`\n",
    "- level (int) – the level number\n",
    "- size (tuple) – 특정 `level`에서의 영역 크기`(width, height)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_to_scaled_pil_image(slide, level):\n",
    "    \"\"\"\n",
    "    Obtain scaled-down PIL image from WSI slide\n",
    "    \"\"\"\n",
    "    \n",
    "    slide_width, slide_height = slide.dimensions\n",
    "    image = slide.read_region((0, 0), level, slide.level_dimensions[level]).convert(\"RGB\")\n",
    "\n",
    "    print(f\"Origial slide size (width, height) : {slide_width}, {slide_height}\")\n",
    "    print(f\"PIL Image size at level {level} : {image.size}\")\n",
    "    print(f\"NumPy array shape at level {level} : {np.array(image).shape}\")\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(참고)\n",
    "\n",
    "`OpenSlide`, `PIL`에서는 이미지 차원을 (width, height) 순으로 표현하고,\n",
    "\n",
    "`OpenCV`, `np.array` 에서는 이미지 차원을 (height, width, channels) 순으로 표현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "pil_img = slide_to_scaled_pil_image(slide, level = 4)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(pil_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tumor Region annotation\n",
    "Whole-slide 이미지에서 종양 영역(tumor region)을 정의하는 어노테이션(annotation)파일을 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "def parse_xml_annotations(annotation_filepath, mask_level):\n",
    "    \"\"\"\n",
    "    Parses the XML annotation file to extract tumor area coordinates.\n",
    "\n",
    "    Args:\n",
    "        annotation_filepath (str): Path to the XML annotation file.\n",
    "        mask_level (int): The level of the mask (resolution level).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of coordinate lists for each annotated tumor area.\n",
    "    \"\"\"\n",
    "    xml_root = parse(annotation_filepath).getroot()\n",
    "    tumor_areas = []\n",
    "\n",
    "    for region in xml_root.iter('Coordinates'):\n",
    "        coordinates = []\n",
    "        for point in region:\n",
    "            x = round(float(point.get('X')) / (2 ** mask_level))\n",
    "            y = round(float(point.get('Y')) / (2 ** mask_level))\n",
    "            coordinates.append([x, y])\n",
    "        tumor_areas.append(coordinates)\n",
    "\n",
    "    return tumor_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tumor region은 `(x, y)` 좌표들의 집합으로 표현되며, 이 점들이 이루는 polygon내부가 tumor영역을 나타냅니다.\n",
    "- `mask_level`값은 슬라이드 이미지의 어느 `level`에 대응하는 `(x ,y)` 좌표들을 담고 있을지 결정합니다.\n",
    "- 일반적으로 mask image는 원본 이미지 (`level = 0`)보다 낮은 `level`에서 작업하여 계산 효율을 높입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "tumor_areas = parse_xml_annotations(annotation_filepath, mask_level = 0)\n",
    "print(f\"Number of areas : {len(tumor_areas)}\")\n",
    "print(f\"Tumor area #1 shape: {np.array(tumor_areas[0]).shape}\")\n",
    "print(f\"Tumor area #1 values: {np.array(tumor_areas[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def mark_tumor_area(slide_thumbnail, mask_level):\n",
    "    \"\"\"\n",
    "    Draw contours of tumor area on given slide_thumbnail with same level\n",
    "    \"\"\"\n",
    "    tumor_areas = parse_xml_annotations(annotation_filepath, mask_level)\n",
    "    \n",
    "    for area in tumor_areas:\n",
    "        cv2.drawContours(image=slide_thumbnail, contours=np.array([area]),\n",
    "                         contourIdx=-1, color=(0, 255, 0), thickness=4)\n",
    "        \n",
    "mask_level = 4\n",
    "slide_thumbnail = np.array(slide_to_scaled_pil_image(slide, level = mask_level))\n",
    "mark_tumor_area(slide_thumbnail, mask_level = mask_level)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(slide_thumbnail)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask image 생성\n",
    "Whole-slide image분석을 위해서는 Tumor mask와 Normal mask 두개의 마스크 이미지가 필요합니다.\n",
    "\n",
    "<img src=\"resources/masks.png\" width=\"600\" align=\"middle\"/>\n",
    "\n",
    "### Tumor mask\n",
    "Tumor영역을 나타내는 0, 255의 값을 가지는 이미지입니다.\n",
    "\n",
    "XML 파일에서 읽은 종양 영역 annotation을 이용하여 그릴 수 있습니다.\n",
    "- `np.zeros`를 이용하여 `mask_level`에 해당하는 마스크 이미지를 초기화 (`dtype=np.uint8`)\n",
    "- 예를들어 특정 `level`에서의 slide 이미지의 크기가 `(width, height) = (100, 200)`이라면 `(200, 100)`의 모양(shape)을 가지는 `np.array` 생성한다.\n",
    "- `cv2.drawContours`함수 ([docs](https://opencv-python.readthedocs.io/en/latest/doc/15.imageContours/imageContours.html))를 이용하여 tumor영역에 `color = 255`값을 채워준다. (`thickness = -1`으로 컨투어 내부를 채워줍니다)\n",
    "\n",
    "### Tissue mask\n",
    "Tissue mask는 slide image중 실제로 조직이 포함된 영역을 표현해줍니다.\n",
    "\n",
    "이번 실습에서는 OpenCV의 Otsu's thresholding 알고리즘을 사용하여 tissue mask를 생성합니다.\n",
    "\n",
    "<img src=\"resources/threshold.png\" width=\"400\" align=\"middle\"/>\n",
    "\n",
    "<img src=\"resources/Otsu.jpg\" width=\"400\" align=\"middle\"/>\n",
    "\n",
    "### Normal mask\n",
    "Normal 조직이 포함된 영역을 표현해주는 0과 255의 값을 가지는 이미지입니다.\n",
    "- `cv2.subtract`함수를 이용하여 Tissue mask에서 Tumor 영역을 빼주면 Normal mask를 얻을 수 있습니다.\n",
    "\n",
    "### <mark>실습</mark> create_masks\n",
    "위 설명을 참고하여 함수 `create_masks`를 완성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(slide_filepath, annotation_filepath, mask_level):\n",
    "    \"\"\"\n",
    "    Creates tumor, normal, and tissue masks using the XML annotations and Otsu's thresholding.\n",
    "\n",
    "    Args:\n",
    "        slide_filepath (str): Path to the slide image file.\n",
    "        annotation_filepath (str): Path to the XML annotation file.\n",
    "        mask_level (int): The level of the mask (resolution level).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the slide at the specified level\n",
    "    slide = openslide.OpenSlide(slide_filepath)\n",
    "    slide_thumbnail  = np.array(slide.read_region((0, 0), mask_level, slide.level_dimensions[mask_level]).convert(\"RGB\"))\n",
    "\n",
    "    # Parse XML annotations to get tumor coordinates\n",
    "    tumor_areas = parse_xml_annotations(annotation_filepath, mask_level)\n",
    "    \n",
    "    # Draw tumor boundaries on the slide thumbnail\n",
    "    for area in tumor_areas:\n",
    "        cv2.drawContours(image=slide_thumbnail, contours=np.array([area]),\n",
    "                         contourIdx=-1, color=(0, 255, 0), thickness=4)\n",
    "    \n",
    "    ##### YOUR CODE START #####\n",
    "    # Initialize empty mask for tumor mask\n",
    "    tumor_mask = None # TODO\n",
    "    # Fill the tumor areas on the tumor mask\n",
    "\n",
    "    ##### YOUR CODE END #####\n",
    "\n",
    "    # Create the tissue mask using Otsu's thresholding on the saturation channel\n",
    "    slide_region = slide.read_region((0, 0), mask_level, slide.level_dimensions[mask_level])\n",
    "    slide_rgb = cv2.cvtColor(np.array(slide_region), cv2.COLOR_RGBA2RGB)\n",
    "    slide_hsv = cv2.cvtColor(slide_rgb, cv2.COLOR_RGB2HSV)\n",
    "    saturation_channel = slide_hsv[:, :, 1] # 채도(Saturation) refers to intensity of colors. Tissue regions are likely to show higher saturation values\n",
    "    _, tissue_mask = cv2.threshold(saturation_channel, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    ##### YOUR CODE START #####\n",
    "    # Create the normal mask by excluding tumor areas from the tissue mask\n",
    "    normal_mask = None # TODO\n",
    "    ##### YOUR CODE END #####\n",
    "\n",
    "    return slide, slide_thumbnail, tumor_mask, tissue_mask, normal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "mask_level = 4  # define resolution level of mask\n",
    "\n",
    "slide, slide_thumbnail, tumor_mask, tissue_mask, normal_mask = create_masks(slide_filepath, annotation_filepath, mask_level = mask_level)\n",
    "print(\"Mask shape: \", tumor_mask.shape)\n",
    "\n",
    "assert len(tumor_mask.shape) == 2, \"Tumor mask should be single channel image\"\n",
    "assert slide_thumbnail.shape[:2] == tumor_mask.shape, \"Tumor mask shape is inconsistant with slide image\"\n",
    "assert tumor_mask.shape == normal_mask.shape, \"Normal mask shape is inconsistant with Tumor mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_masks(slide_thumbnail, tumor_mask, tissue_mask, normal_mask):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    images = [slide_thumbnail, tumor_mask, tissue_mask, normal_mask]\n",
    "    titles = ['Slide Thumbnail', 'Tumor Mask', 'Tissue Mask', 'Normal Mask']\n",
    "\n",
    "    for ax, img, title in zip(axes.ravel(), images, titles):\n",
    "        cmap = 'gray' if len(img.shape) == 2 else None  # Use grayscale colormap for 2D masks\n",
    "        ax.imshow(img, cmap=cmap, vmin=0, vmax=255)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "draw_masks(slide_thumbnail, tumor_mask, tissue_mask, normal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch extraction\n",
    "Whole slide image는 크기가 매우 크므로 (주로 gigapixel) 딥러닝에 활용하기 위해서는 패치(patch)라 불리는 적당한 크기의 (예: 256x256 픽셀) 이미지로 분할하는 과정이 필요합니다.\n",
    "\n",
    "<img src=\"resources/patches.png\" width=\"400\" align=\"middle\"/>\n",
    "\n",
    "- patch는 `level = 0`의 원본 이미지에서 추출합니다.\n",
    "- 해당 patch가 tumor에 해당되는지 normal에 해당되는지 여부는 각각 이에 대응하는 `tumor_mask`와 `normal_mask`의 픽셀 값을 이용하여 판단합니다.\n",
    "- 이때, mask 이미지와 원본 이미지의 해상도가 다르다는 것에 주의해야 합니다 (아래 이미지 참고).\n",
    "\n",
    "<img src=\"resources/mask_step_size.jpg\" width=\"400\" align=\"middle\"/>\n",
    "\n",
    "\n",
    "### <mark>실습</mark> extract_patches\n",
    "`extract_patches`함수를 완성하세요\n",
    "- `slide.read_region`함수를 사용하여 원본 이미지로 부터 패치를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "def extract_patches(slide, slide_thumbnail, tumor_mask, normal_mask, mask_level, patch_config = dict(), save_path = None):\n",
    "    \"\"\"\n",
    "    Extracts normal and tumor patches from the slide using the provided masks.\n",
    "\n",
    "    Args:\n",
    "        slide (OpenSlide object): The whole slide image.\n",
    "        slide_thumbnail (numpy array): The slide thumbnail image.\n",
    "        tumor_mask (numpy array): The mask of tumor areas.\n",
    "        normal_mask (numpy array): The mask of normal tissue areas.\n",
    "        mask_level (int): The level of the mask (resolution level).\n",
    "        patch_config (dict): Dictionary containing configuration for patch extraction.\n",
    "        save_path (str, optional): Directory to save extracted patches. Defaults to None.\n",
    "       \"\"\"\n",
    "\n",
    "    patch_size = patch_config.get('patch_size', 304)  # Patch size at the highest resolution\n",
    "    \n",
    "    normal_area_threshold = patch_config.get('normal_area_threshold', 0.1) # normal mask inclusion ratio that select normal patches\n",
    "    normal_sel_ratio = patch_config.get('normal_sel_ratio', 1) # nomral patch selection ratio \n",
    "    max_normal_patches = patch_config.get('max_normal_patches', 1000) # number limit of normal patches \n",
    "\n",
    "    tumor_area_threshold = patch_config.get('tumor_area_threshold', 0.8) # tumor mask inclusion ratio that select tumor patches\n",
    "    tumor_sel_ratio = patch_config.get('tumor_sel_ratio', 1) # tumor patch selection ratio\n",
    "    max_tumor_patches = patch_config.get('max_tumor_patches', 1000) # number limit of tumor patches\n",
    "\n",
    "\n",
    "    downsample_factor = 2 ** mask_level\n",
    "    mask_step_size = patch_size // downsample_factor  # Step size at the mask level\n",
    "\n",
    "    slide_width, slide_height = slide.level_dimensions[0]\n",
    "    num_patches_x = slide_width // patch_size\n",
    "    num_patches_y = slide_height // patch_size\n",
    "    total_patches = num_patches_x * num_patches_y\n",
    "\n",
    "    tumor_patches_extracted = 0\n",
    "    normal_patches_extracted = 0\n",
    "    patches_processed = 0\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            rand = random.random()\n",
    "\n",
    "            x_mask = i * mask_step_size\n",
    "            y_mask = j * mask_step_size\n",
    "            x_slide = i * patch_size\n",
    "            y_slide = j * patch_size\n",
    "\n",
    "            ##### YOUR CODE START #####\n",
    "            # Extract corresponding mask regions\n",
    "            tumor_mask_region = None    # TODO\n",
    "            normal_mask_region = None  # TODO\n",
    "            mask_area = mask_step_size * mask_step_size * 255\n",
    "            \n",
    "            tumor_area_ratio = None    # TODO\n",
    "            normal_area_ratio = None  # TODO\n",
    "\n",
    "            # Extract tumor patches\n",
    "            if (tumor_area_ratio > tumor_area_threshold) and (rand <= tumor_sel_ratio) and (tumor_patches_extracted < max_tumor_patches):\n",
    "                patch = None # TODO\n",
    "                if save_path:\n",
    "                    patch.save(f\"{save_path}/t_{str(i)}_{str(j)}.png\")\n",
    "                cv2.rectangle(slide_thumbnail, (x_mask, y_mask), (x_mask + mask_step_size, y_mask + mask_step_size), (0, 0, 255), 2)\n",
    "                tumor_patches_extracted += 1\n",
    "            \n",
    "            # Extract normal patches\n",
    "            elif (normal_area_ratio > normal_area_threshold) and (tumor_area_ratio == 0) and (rand <= normal_sel_ratio) and (normal_patches_extracted < max_normal_patches):\n",
    "                patch = None # TODO\n",
    "                if save_path:\n",
    "                    patch.save(f\"{save_path}/n_{str(i)}_{str(j)}.png\")\n",
    "                cv2.rectangle(slide_thumbnail, (x_mask, y_mask), (x_mask + mask_step_size, y_mask + mask_step_size), (255, 255, 0), 2)\n",
    "                normal_patches_extracted += 1\n",
    "            \n",
    "            patches_processed += 1\n",
    "            ##### YOUR CODE END #####\n",
    "\n",
    "    print(f'Processed {patches_processed}/{total_patches} patches.')\n",
    "    print(f'Extracted {tumor_patches_extracted} tumor patches and {normal_patches_extracted} normal patches.')\n",
    "\n",
    "\n",
    "def draw_patches(patch, slide_thumbnail_patch, tumor_mask_patch):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    images = [patch, slide_thumbnail_patch, tumor_mask_patch]\n",
    "    titles = ['Patch', 'Slide Thumbnail', 'Tumor Mask']\n",
    "\n",
    "    for ax, img, title in zip(axes.ravel(), images, titles):\n",
    "        cmap = 'gray' if len(img.shape) == 2 else None  # Use grayscale colormap for 2D masks\n",
    "        ax.imshow(img, cmap=cmap, vmin=0, vmax=255)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "mask_level = 4  # define resolution level of mask\n",
    "\n",
    "slide, slide_thumbnail, tumor_mask, tissue_mask, normal_mask = create_masks(slide_filepath, annotation_filepath, mask_level = mask_level)\n",
    "extract_patches(slide, slide_thumbnail, tumor_mask, normal_mask, mask_level = mask_level, save_path = None)\n",
    "\n",
    "plt.figure(figsize = (16,16))\n",
    "plt.imshow(slide_thumbnail)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up saved patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('patches/'):\n",
    "    shutil.rmtree('patches/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a deep learning model\n",
    "## PatchCamelyon (PCam) 데이터셋\n",
    "\n",
    "PatchCamelyon 데이터셋은 Camelyon16 데이터셋으로 부터 (96 x 96px) 크기의 patch를 추출해놓은 데이터셋이며, 유방암 환자의 림프절(lymph node) 병리(histopathology) 이미지로 부터 추출된 327,680개의 RGB 컬러 이미지로 구성되어 있습니다.\n",
    "\n",
    "**Label (Target)**\n",
    "PCam 데이터셋의 라벨(target)은 0 (normal), 1(tumor)이며, 림프절에서의 암 존재 여부를 분류하는 딥러닝 모델을 개발/평가하는데 사용 될 수 있습니다.\n",
    "- 만약 patch 중심의 (32 x 32px)영역중 한 개 이상의 pixel이 종양 조직이면 1의 라벨을 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from training_utilities import train_loop, evaluation_loop, create_dataloaders, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pcam_datasets(data_root_dir):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.PCAM(root=data_root_dir, split = \"train\", transform=train_transforms)\n",
    "    val_dataset = datasets.PCAM(root=data_root_dir, split = \"val\", transform=test_transforms)\n",
    "    test_dataset = datasets.PCAM(root=data_root_dir, split = \"test\", transform=test_transforms)\n",
    "\n",
    "    num_classes = 2\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch `Dataset`은 데이터(feature)와 타겟(target, label)을 한번에 하나씩 가져오는 기능을 제공한다.\n",
    "\n",
    "index를 전달받으면 그 index에 대응하는 데이터(이미지/feature)와 target을 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data_root_dir = \"/datasets\"\n",
    "train_dataset, val_dataset, test_dataset, num_classes = load_pcam_datasets(data_root_dir)\n",
    "\n",
    "print(\"Train size: \", len(train_dataset))\n",
    "print(\"Validation size: \", len(val_dataset))\n",
    "print(\"Test size: \", len(test_dataset))\n",
    "print(\"Image shape: \", train_dataset[0][0].shape)\n",
    "print(\"Label of fisrt example: \", train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_few_samples(dataset, cols=8, rows=5, shuffle = False):\n",
    "    label_names = ['normal', 'tumor']\n",
    "\n",
    "    if shuffle:\n",
    "        sample_indices = np.random.randint(0, len(dataset), size=cols * rows)\n",
    "    else:\n",
    "        sample_indices = list(range(cols * rows))\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    \n",
    "    figure, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2)) \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        img, label = dataset[sample_idx]\n",
    "        img = img * std + mean   # Unnormalize to [0,1] for display\n",
    "        img = img.permute(1, 2, 0).numpy()  # CHW to HWC, to numpy\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(label_names[label])\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "visualize_few_samples(train_dataset, cols = 5, rows = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stain variation\n",
    "이미지를 살펴보면 이미지 별로 색상이 차이가 있는것을 확인할 수 있다.\n",
    "\n",
    "이는 병원마다 H&E 염색을 수행하는 프로토콜이 서로 다르기 때문입니다. 딥러닝 모델을 학습할 때 이러한 변이에 일반화(generalization)할 수 있도록 학습 과정을 설계하는 것이 중요합니다.\n",
    "\n",
    "1. **Stain normalization methods**\n",
    "\n",
    "염색 차이를 보정하기 위해 Stain Normalization 기법을 사용할 수 있습니다.\n",
    "\n",
    "<img src=\"resources/stain_normalization.png\" width=\"500\" align=\"middle\"/>\n",
    "\n",
    "Macenko's Algorithm : SVD (Singular value decomposition)을 이용해 hematoxylin 채널과 eosin채널을 분리하고, 이를 정규화(normalize) 하는 방법입니다\n",
    "\n",
    "<img src=\"resources/stain_normalization_Macenko.jpg\" width=\"700\" align=\"middle\"/>\n",
    "\n",
    "단점\n",
    "- 특정 기관이나 데이터셋을 대표한다고 여겨지는 참조 이미지(Reference Image)에 의존하므로, 일반화 성능이 떨어질 수 있습니다.\n",
    "- stain artifact에 불안정할 수 있습니다..\n",
    "\n",
    "최근에는 보다 안정적인 결과를 얻기 위해 GAN과 같은 딥러닝 기반의 Stain Normalization 방법이 널리 사용되고 있습니다.\n",
    "\n",
    "2. **Data Augmentation**\n",
    "\n",
    "모델이 다양한 stain variation에 대해 일반화될 수 있도록, 학습 과정에서 다양한 stain Variation을 가진 이미지에 노출시키는 방법입니다.\n",
    "\n",
    "- 이번 실습에서는 `ColorJitter`를 사용하여 색상, 밝기, 대비, 채도를 변형하는 Data Augmentation을 수행합니다.\n",
    "- 단, 데이터가 부족하거나 stain variation을 지나치게 줄 경우 학습이 더 어려워 질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pcam_datasets(data_root_dir):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.PCAM(root=data_root_dir, split = \"train\", transform=train_transforms)\n",
    "    val_dataset = datasets.PCAM(root=data_root_dir, split = \"val\", transform=test_transforms)\n",
    "    test_dataset = datasets.PCAM(root=data_root_dir, split = \"test\", transform=test_transforms)\n",
    "\n",
    "    num_classes = 2\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset, num_classes = load_pcam_datasets(data_root_dir)\n",
    "visualize_few_samples(train_dataset, cols = 5, rows = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "딥러닝 학습에서는 샘플들을 주로 \"mini-batches\"로 가져온고, 매 epoch마다 랜덤하게 섞어주며, multiprocessing을 사용해 데이터 획득을 빠르게 합니다.\n",
    "\n",
    "DataLoader이 복잡한 과정을 쉽도록 도와준다.\n",
    "\n",
    "- `DataLoader`는 `Dataset`을 배치 단위로 묶어준다 (`batch_size`).\n",
    "- `shuffle`를 통해 매 epoch마다 랜덤하게 섞어주는 기능을 제공한다\n",
    "- `num_workers`를 통해 데이터 전처리를 multiprocessing으로 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 2)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}, dtype: {y.dtype}\")\n",
    "    print(f\"Target y values: {y}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, config):\n",
    "    if model_name == \"resnet50\":\n",
    "        if config.get('pretrained', \"\"): #if pretrained model name is given\n",
    "            print(f'Using pretrained model {config[\"pretrained\"]}')\n",
    "            model = models.resnet50(weights = config[\"pretrained\"])\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "                \n",
    "        else:\n",
    "            model = models.resnet50()\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif model_name == \"densenet121\":\n",
    "        if config.get('pretrained', \"\"): #if pretrained model name is given\n",
    "            print(f'Using pretrained model {config[\"pretrained\"]}')\n",
    "            model = models.densenet121(weights = config[\"pretrained\"]) \n",
    "        else:\n",
    "            model = models.densenet121()\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Model not supported: {}\".format(model_name))\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Using model {model_name} with {total_params} parameters ({trainable_params} trainable)\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(config):\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = config['data_root_dir']\n",
    "    num_worker = config.get('num_worker', 4)\n",
    "\n",
    "    ## Hyper parameters\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    num_epochs = config['num_epochs']\n",
    "\n",
    "    ## checkpoint setting\n",
    "    checkpoint_save_interval = config.get('checkpoint_save_interval', 10)\n",
    "    checkpoint_path = config.get('checkpoint_path', \"checkpoints/checkpoint.pth\")\n",
    "    best_model_path = config.get('best_model_path', \"checkpoints/best_model.pth\")\n",
    "    load_from_checkpoint = config.get('load_from_checkpoint', None)\n",
    "\n",
    "    ## variables\n",
    "    best_acc1 = 0\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset, num_classes = load_pcam_datasets(data_root_dir)\n",
    "    \n",
    "    train_dataloader, val_dataloader, test_dataloader = create_dataloaders(train_dataset, val_dataset, test_dataset, device, \n",
    "                                                           batch_size = batch_size, num_worker = num_worker)\n",
    "        \n",
    "    model = get_model(model_name = config[\"model_name\"], num_classes= num_classes, config = config).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) \n",
    "\n",
    "    if load_from_checkpoint:\n",
    "        load_checkpoint_path = (best_model_path if load_from_checkpoint == \"best\" else checkpoint_path)\n",
    "        start_epoch, best_acc1 = load_checkpoint(load_checkpoint_path, model, optimizer, scheduler, device)\n",
    "\n",
    "    if config.get('test_mode', False):\n",
    "        # Only evaluate on the test dataset\n",
    "        print(\"Running test evaluation...\")\n",
    "        test_acc = evaluation_loop(model, device, test_dataloader, criterion, phase = \"test\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "        \n",
    "    else:\n",
    "        # Train and validate using train/val datasets\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            train_loop(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "            val_acc1 = evaluation_loop(model, device, val_dataloader, criterion, epoch = epoch, phase = \"validation\")\n",
    "            scheduler.step()\n",
    "\n",
    "            if (epoch + 1) % checkpoint_save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "                is_best = val_acc1 > best_acc1\n",
    "                best_acc1 = max(val_acc1, best_acc1)\n",
    "                save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, best_acc1, is_best, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_root_dir': '/datasets',\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_epochs': 1,\n",
    "    'model_name': 'densenet121',\n",
    "    'pretrained' : 'IMAGENET1K_V1',\n",
    "\n",
    "    \"checkpoint_save_interval\" : 1,\n",
    "    \"checkpoint_path\" : \"checkpoints/checkpoint.pth\",\n",
    "    \"best_model_path\" : \"checkpoints/best_model.pth\",\n",
    "    \"load_from_checkpoint\" : None,    # Options: \"latest\", \"best\", or None\n",
    "}\n",
    "train_main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "config_testmode = {\n",
    "    **config, \n",
    "    'test_mode': True, # True if evaluating only test set\n",
    "    'load_from_checkpoint': 'best'\n",
    "}\n",
    "\n",
    "train_main(config_testmode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>실습</mark> .gitignore\n",
    "모델 checkpoint를 git에 올리지 않기 위에 `.gitignore`를 수정하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab을 마무리 짓기 전 저장된 checkpoint를 모두 지워 저장공간을 확보한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.exists('checkpoints/'):\n",
    "    shutil.rmtree('checkpoints/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openslide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
